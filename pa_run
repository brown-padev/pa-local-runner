#!/usr/bin/env python3

import os
import sys
import json
import signal
import shutil
import pathlib
import argparse
import tempfile
import subprocess

import dataclasses
from dataclasses import dataclass

import colors as c

VERBOSE_MODE = False
SCRIPT_PATH = pathlib.Path(os.path.realpath(__file__)).parent.resolve()
PA_GRADING_RUNNER = SCRIPT_PATH / "pa_notes.php"
JSON_RESULTS_DEFAULT = pathlib.Path("/tmp/results.json")
DEFAULT_WORK_DIR = pathlib.Path("/tmp/pa_run")

STATUS_PASS = "passed"
STATUS_FAIL = "failed"

from pa_results import PaGradeEntry, PaRunner, PaConfig, PATestEntry, PAResults

def _status(ok):
    if ok:
        return c.OKGREEN + "SUCCESS" + c.ENDC
    else:
        return c.FAIL + "FAILED" + c.ENDC

def _prel(p: str | pathlib.Path):
    _p = p if isinstance(p, pathlib.Path) else pathlib.Path(p)
    cwd = pathlib.Path(os.getcwd())
    if _p.is_relative_to(cwd):
        return _p.relative_to(cwd)
    else:
        return _p


def _prels(p: str | pathlib.Path):
    return str(_prel(p))


def msg(m, step=None, color=None):
    c_start = "" if color is None else color
    c_end = "" if color is None else c.ENDC
    print("{}{}{}".format(c_start, m, c_end))

def print_status(component, message):
    bc = c.OKCYAN
    print("{}[{}]{} {}".format(bc, component, c.ENDC, message))


def do_exit(message):
    print(message)
    sys.exit(1)

def mkdir(dir_name):
    pathlib.Path.mkdir(pathlib.Path(dir_name),
                       exist_ok=True, parents=True)


class RunConfig():
    grading_dir: pathlib.Path
    target_dir: pathlib.Path
    overlay_dir: pathlib.Path
    expected_dir: pathlib.Path
    _pa_config: PaConfig
    use_overlay: bool
    pset_config: pathlib.Path | None
    grading_script: pathlib.Path | None

    CONFIG_NAME = ".parunconfig.json"

    def __init__(self,
                 grading_dir,
                 target_dir=".",
                 expected_dir=None,
                 overlay_dir=None,
                 use_overlay=True,
                 pset_config=None,
                 grading_script=None):

        self.grading_dir = pathlib.Path(grading_dir).resolve()
        self.target_dir = pathlib.Path(target_dir)

        def _rp(val, default):
            return pathlib.Path(val).resolve() if val is not None else default

        self.expected_dir = _rp(expected_dir, self.grading_dir / "expected")
        self.use_overlay = use_overlay
        self.overlay_dir = _rp(overlay_dir, self.grading_dir / "overlay")

        self.pset_config = pset_config

        if self.pset_config is None:
            self.pset_config = self.grading_dir / "config.json"

        self._pa_config = PaConfig.from_json_file(self.pset_config)

        self.grading_script = grading_script
        if self.grading_script is None:
            _grading_script = self.grading_dir / "grade.php"
            if _grading_script.exists():
                self.grading_script = _grading_script


    def all_commands(self) -> list[str]:
        return list(self._pa_config.runners.keys())

    def has_command(self, command: str) -> bool:
        return command in self.all_commands()

    def get_runner(self, command) -> PaRunner:
        if not self.has_command(command):
            raise ValueError(f"No runner named f{command}")

        return self._pa_config.runners[command]

    @classmethod
    def load_from_file(cls, config_file):
        with open(config_file, "r") as json_fd:
            jd = json.load(json_fd)
            return RunConfig(**jd)

    @classmethod
    def load_config(cls, start_dir="."):
        dir_to_check = pathlib.Path(start_dir).resolve()
        if dir_to_check.is_file():
            return cls.load_from_file(str(dir_to_check))

        config = None
        while True:
            config_path = dir_to_check / cls.CONFIG_NAME
            if config_path.exists():
                config = cls.load_from_file(config_path)
            dir_to_check = dir_to_check.parent
            if str(dir_to_check) == dir_to_check.root:
                break

        if config is None:
            import pdb; pdb.set_trace()
            raise ValueError("No config found")

        return config


def has_cmd(cmd_name):
    output, rv = do_exec("which {}".format(cmd_name), shell=True, check=False, return_rv=True)
    return rv == 0


def do_exec(cmd, check=True, shell=True, cwd=None, return_rv=False):
    global VERBOSE_MODE

    if VERBOSE_MODE:
        msg("Executing:  {}".format(" ".join(cmd) if isinstance(cmd, list) else cmd))

    proc = subprocess.run(" ".join(cmd) if shell and isinstance(cmd, list) else cmd, shell=shell, text=True,
                          stdout=subprocess.PIPE, stderr=subprocess.STDOUT, cwd=cwd)

    if check and proc.returncode != 0:
        do_exit(f"Command exited with {proc.returncode}:  {proc.stdout}")

    output = proc.stdout
    if return_rv:
        return output, proc.returncode
    else:
        return output

def do_exec_live(cmd, log_file=None, shell=True, check=False, attach=False, cwd=None):
    global VERBOSE_MODE

    if VERBOSE_MODE:
        msg("Executing:  {}".format(" ".join(cmd) if isinstance(cmd, list) else cmd))

    def _become_tty_fg():
        os.setpgrp()
        hdlr = signal.signal(signal.SIGTTOU, signal.SIG_IGN)
        tty = os.open("/dev/tty", os.O_RDWR)
        os.tcsetpgrp(tty, os.getpgrp())
        signal.signal(signal.SIGTTOU, hdlr)

    if (log_file is None) and (not attach):
        raise ValueError("Log file must be defined for non-interactive process")

    log_fd = open(str(log_file), "wb") if (not attach) else None

    cmd_to_run = " ".join(cmd) if shell and isinstance(cmd, list) else cmd
    kwargs = {}
    if attach:
        kwargs["preexec_fn"] = _become_tty_fg
    else:
        kwargs["stdout"] = subprocess.PIPE
        kwargs["stderr"] = subprocess.STDOUT
        kwargs["bufsize"] = 128

    proc = subprocess.Popen(cmd_to_run, shell=shell, cwd=cwd,
                            **kwargs)

    if (not attach):
        for line in proc.stdout:
            sys.stdout.buffer.write(line)
            log_fd.write(line)

    proc.wait()
    if log_fd is not None:
        log_fd.close()

    if check and proc.returncode != 0:
        do_exit(f"Command exited with {proc.returncode}:  {proc.stdout}")

    return proc


class RunInstance():
    config: RunConfig
    submission_dir: pathlib.Path
    work_path: pathlib.Path
    work_tempfile: tempfile.TemporaryDirectory | None = None
    preserve_work_dir: bool = False
    output_dir: pathlib.Path
    log_file: pathlib.Path
    notes_json: pathlib.Path
    results_json: pathlib.Path
    runner_json: pathlib.Path
    expected_results: pathlib.Path | None
    runner_rv: int

    RV_UNSET = -999

    def __init__(self,
                 config: RunConfig,
                 submission_dir: pathlib.Path,
                 work_dir: pathlib.Path | None=None,
                 output_dir: pathlib.Path | None=None,
                 expected_results: pathlib.Path | None = None,
                 preserve_work_dir=False):
        self.config = config
        self.submission_dir = submission_dir

        self.preserve_work_dir = preserve_work_dir
        if work_dir is None:
            #self.work_tempfile = tempfile.TemporaryDirectory(delete=(not preserve_work_dir))
            #self.work_path = pathlib.Path(self.work_tempfile.name)
            self.work_path = DEFAULT_WORK_DIR
        else:
            self.work_path = pathlib.Path(work_dir)

        if output_dir is not None:
            self.output_dir = output_dir
        else:
            self.output_dir = self.work_path / "output"

        self.expected_results = pathlib.Path(expected_results) if expected_results is not None else None

        self.log_file = self.output_dir / "output.log"
        self.notes_json = self.output_dir / "pa_notes.json"
        self.results_json = self.output_dir / "results.json"
        self.runner_json = JSON_RESULTS_DEFAULT

        if self.work_path == DEFAULT_WORK_DIR:
            shutil.rmtree(str(self.work_path))

        mkdir(str(self.work_path))
        self.clear_output_files()
        self.runner_rv = self.RV_UNSET

    def dir_name(self):
        return self.submission_dir.resolve().stem

    def cleanup(self):
        # if self.work_tempfile and (not self.preserve_work_dir):
        #     self.work_tempfile.cleanup()
        pass

    def clear_output_files(self):
        self.notes_json.unlink(missing_ok=True)
        self.runner_json.unlink(missing_ok=True)
        self.log_file.unlink(missing_ok=True)
        self.results_json.unlink(missing_ok=True)

    def _exec(self, cmd, check=True, shell=True):
        return do_exec(cmd, check=check, shell=shell, cwd=str(self.work_path))

    def get_expected_file(self, command=None):
        if self.expected_results is not None:
            assert (self.expected_results.exists())
            return self.expected_results

        if command is None:
            raise ValueError("Must specify command to search for expected output")

        target = self.config.expected_dir / self.dir_name() / "{}.json".format(command)
        return target

    def get_run_command(self, command):
        runner = self.config.get_runner(command)
        return runner.command

    def do_run(self, command):
        root_work_path = pathlib.Path("repo")
        #pset_path = root_work_path / self.config._pa_config.directory
        pset_path = root_work_path / str(self.config.target_dir)
        self._exec(f"mkdir -pv {str(pset_path)}")
        self._exec(f"mkdir -pv {str(self.output_dir)}")
        self._exec(f"cp -pTRv {self.submission_dir} {str(pset_path)}")

        if self.config.use_overlay:
            self._exec(f"cp -pTRv {str(self.config.overlay_dir)} {str(root_work_path)}")
            self._exec("touch config.sh")
            self._exec("touch config.mk")

        runner = self.config.get_runner(command)
        if runner.command is None:
            import pdb; pdb.set_trace()
            raise ValueError("YO")

        interactive = runner.is_interactive()
        proc = do_exec_live(runner.command, str(self.log_file), shell=True,
                            cwd=str(self.work_path), attach=interactive)
        rv = proc.returncode
        if rv != 0 or VERBOSE_MODE:
            msg("Runner exited with status {}".format(rv))
            msg("Checkoff run '{}':  {}".format(command, _status(rv == 0)))

        return rv

    def runner_ok(self):
        assert (self.runner_rv != self.RV_UNSET)
        return (self.runner_rv == 0) or (self.runner_json.exists())

    def do_notes(self, command):
        grading_script = self.config.grading_script
        if grading_script is None:
            msg("[STEP 2] No grading script for this pset, skipping notes pass".format(command),
                color=c.WARNING)
            return

        runner = self.config.get_runner(command)
        if runner.eval is None:
            msg("[STEP 2] No grading function found for {}, skipping notes pass".format(command),
                color=c.OKBLUE)
            return

        cmd = [
            "php",
            str(PA_GRADING_RUNNER),
            f"-p{str(self.config.pset_config)}",
            f"-f{str(self.log_file)}",
            f"-o{str(self.notes_json)}",
            str(self.config.grading_script),
            "'{}'".format(runner.eval),
        ]
        with tempfile.NamedTemporaryFile() as log_fd:
            do_exec_live(cmd, log_fd.name, shell=True,
                         cwd=str(self.work_path), attach=False, check=True)
        if not self.notes_json.exists():
            msg("WARNING:  No notes file found at {}".format(str(self.notes_json)),
                color=c.WARNING)

    def do_process_results(self, command):

        runner_results = self.runner_json
        if runner_results.exists():
            results = PAResults.from_json_file(runner_results)
        elif not self.runner_ok():
            results = PAResults.from_log(self.log_file, self.runner_rv)
            if VERBOSE_MODE:
                msg("Warning:  no per-test results found, creating from log file due to error")
        else:
            msg("Warning:  no per-test results found, can only compare based on exit status")
            assert (self.runner_rv != self.RV_UNSET)
            results = PAResults.from_empty()
            results.runner_rv = self.runner_rv

        results.add_grading_rubric(self.config._pa_config.grades)

        if self.notes_json.exists():
            results.add_notes_file(self.notes_json)

        results.write_json(self.results_json)
        if VERBOSE_MODE:
            msg("Wrote results to {}".format(str(self.results_json)))

        return results

    def run_checkoff(self, command):
        print_status("checkoff", "Running command {}".format(command))
        rv = self.do_run(command)
        self.runner_rv = rv
        print_status("checkoff", "Checkoff phase completed, runner exited with status {}".format(self.runner_rv))

        print_status("notes", "Starting notes pass")
        self.do_notes(command)
        print_status("notes", "Notes pass completed")

        results = self.do_process_results(command)

        # Default status:  did the runner succeed?
        run_ok = self.runner_ok()

        return results, run_ok

    @classmethod
    def do_check_expected(cls,
                          test_results_file: pathlib.Path,
                          expected_file: pathlib.Path,
                          show=True, error_on_fail=True):
        diff_output = None
        rv = None

        if not test_results_file.exists():
            msg("FAILURE:  Runner did not produce test results file at {}.  Runner may have aborted before it could write results.".format(str(test_results_file)), c.FAIL)
            return False

        diff_cmd = [
            "diff", "-urN", str(test_results_file), str(expected_file),
        ]

        diff_output, rv = do_exec(diff_cmd, check=False, shell=True, return_rv=True)
        ok = (rv == 0)
        if show:
            if ok:
                msg("{}:  Results match expected results".format(_status(rv == 0)))
            else:
                msg("{}{}{}:  Test results did not match expected results"
                    .format(c.FAIL if error_on_fail else c.WARNING,
                            "ERROR" if error_on_fail else "WARNING",
                            c.ENDC))
                msg(diff_output)
                msg("Expected results: {}".format(str(expected_file)))
                msg("Results this run: {}".format(str(test_results_file)))

        return ok

def compare_results(actual_json: pathlib.Path, expected_json: pathlib.Path):
    assert(actual_json.exists())
    assert(expected_json.exists())

    r_actual = PAResults.from_json_file(actual_json)
    r_expected = PAResults.from_json_file(expected_json)


CHECK_EXPECTED_NONE = "skip"
CHECK_EXPECTED_WARN = "warn"
CHECK_EXPECTED_ERROR = "error"
CHECK_EXPECTED_CHOICES = [
    CHECK_EXPECTED_NONE,
    CHECK_EXPECTED_WARN,
    CHECK_EXPECTED_ERROR,
]

def main(input_args):
    global VERBOSE_MODE

    parser = argparse.ArgumentParser()
    parser.add_argument("--verbose", action="store_true")
    parser.add_argument("--run-config", type=str, default=None)
    parser.add_argument("--no-overlay", action="store_true")
    parser.add_argument("--work-dir", type=str, default=None)
    parser.add_argument("--output-dir", type=str, default=None)
    parser.add_argument("--no-cleanup", action="store_true", default=False)
    parser.add_argument("--expected", type=str, default=None)
    parser.add_argument("--check-expected", type=str,
                        choices=CHECK_EXPECTED_CHOICES, default=CHECK_EXPECTED_ERROR)
    parser.add_argument("--save-expected", action="store_true", default=False)
    parser.add_argument("submission_dir", type=str, default=None)
    parser.add_argument("command", type=str, default="list")

    args = parser.parse_args(input_args)

    submission_path = pathlib.Path(args.submission_dir).resolve()
    start_dir = args.run_config if args.run_config is not None else submission_path
    config = RunConfig.load_config(start_dir)

    if args.verbose:
        VERBOSE_MODE = True

    if args.no_overlay:
        config.use_overlay = False

    command = args.command
    save_expected = args.save_expected
    available_commands = config.all_commands()

    if command == "list" or command not in available_commands:
        print("Available commands:\n{}".format("\n".join(["\t{}".format(c) for c in available_commands])))
        sys.exit(1)

    if args.submission_dir is None:
        print("Must specify submission dir")
        sys.exit(1)

    def _setup_dir(d):
        if d is not None:
            _d = pathlib.Path(d).resolve()
            mkdir(_d)
            return _d
        else:
            return None

    work_dir = _setup_dir(args.work_dir)
    output_dir = _setup_dir(args.output_dir)
    expected_results = pathlib.Path(args.expected) if args.expected else None
    expected_status = args.check_expected

    run = RunInstance(config, submission_path,
                      work_dir=work_dir,
                      output_dir=output_dir,
                      expected_results=expected_results,
                      preserve_work_dir=args.no_cleanup)
    ok = True
    try:
        results, run_ok = run.run_checkoff(command)
        results.show()

        expected_file = run.get_expected_file(command)

        msg("\n++++++++++++++++++++++ PA_RUN SUMMARY ++++++++++++++++++++++", color=c.OKCYAN)
        if save_expected:
            msg("Writing results to expected file {}".format(_prels(expected_file)),
                color=c.OKCYAN)
            mkdir(expected_file.parent)
            print(do_exec(["cp", "-v", str(run.runner_json), str(expected_file)]))
            #results.write_json(expected_file)

        results.show()

        need_expected = (expected_status != CHECK_EXPECTED_NONE)

        def _print_set_expected():
            print("If you believe the current test results are accurate, you can set the expected results by copying the test results to the repo at the appropriate path, e.g.:\n"
                    "\t{}\n"
                    "\tcp -v {} {}\n"
                    .format(run.get_run_command(command),
                            JSON_RESULTS_DEFAULT, _prels(expected_file)))

        if run_ok and expected_status != CHECK_EXPECTED_NONE:
            if not expected_file.exists():
                msg("No expected file found at {}, cannot compare to baseline".format(_prels(expected_file)), color=c.WARNING)
                ok = False
                msg("{}:  Runner exited with status {}, but no expected results found".format(_status(ok), run.runner_rv))
                _print_set_expected()
                #"Use --save-expected to save current results as baseline"
            else:
                show_expected = run_ok
                error_on_fail = (expected_status == CHECK_EXPECTED_ERROR)
                expected_ok = run.do_check_expected(run.runner_json, expected_file,
                                                    show=show_expected,
                                                    error_on_fail=error_on_fail)
                if expected_ok:
                    print("{}:  Test results match expected results".format(_status(expected_ok)))
                else:
                    print("{}:  Test results differ from expected results".format(_status(expected_ok)))
                    _print_set_expected()

                if error_on_fail:
                    ok = ok and expected_ok
                else:
                    print("Expected-results check set in warning-only mode, not returning failure")

        if not need_expected:
            msg("{}:  Runner exited with status {}; no expected test results to check"
                .format(_status(run_ok), run.runner_rv))
            ok = ok and run_ok

    finally:
        run.cleanup()

    msg("++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++", color=c.OKCYAN)
    return 0 if ok else 1

if __name__ == "__main__":
    sys.exit(main(sys.argv[1:]))
