#!/usr/bin/env python3

import os
import sys
import json
import signal
import shutil
import pathlib
import argparse
import tempfile
import subprocess

import dataclasses
from dataclasses import dataclass

import colors as c

VERBOSE_MODE = False
SCRIPT_PATH = pathlib.Path(os.path.realpath(__file__)).parent.resolve()
PA_GRADING_RUNNER = SCRIPT_PATH / "pa_notes.php"
JSON_RESULTS_DEFAULT = pathlib.Path("/tmp/results.json")
DEFAULT_WORK_DIR = pathlib.Path("/tmp/pa_run")
COMPARE_RESULTS_NAME = "compare_results.json"

STATUS_PASS = "passed"
STATUS_FAIL = "failed"

from pa_results import PaGradeEntry, PaRunner, PaConfig, PATestEntry, PAResults
from compare import CompareResult

def _status(ok):
    if ok:
        return c.OKGREEN + "SUCCESS" + c.ENDC
    else:
        return c.FAIL + "FAILED" + c.ENDC

def _prel(p: str | pathlib.Path):
    _p = p if isinstance(p, pathlib.Path) else pathlib.Path(p)
    cwd = pathlib.Path(os.getcwd())
    if _p.is_relative_to(cwd):
        return _p.relative_to(cwd)
    else:
        return _p


def _prels(p: str | pathlib.Path):
    return str(_prel(p))


def msg(m, step=None, color=None):
    c_start = "" if color is None else color
    c_end = "" if color is None else c.ENDC
    print("{}{}{}".format(c_start, m, c_end))

def print_status(component, message):
    bc = c.OKCYAN
    print("{}[{}]{} {}".format(bc, component, c.ENDC, message))


def do_exit(message):
    print(message)
    sys.exit(1)

def mkdir(dir_name):
    pathlib.Path.mkdir(pathlib.Path(dir_name),
                       exist_ok=True, parents=True)


class RunConfig():
    grading_dir: pathlib.Path
    target_dir: pathlib.Path
    overlay_dir: pathlib.Path
    expected_dir: pathlib.Path
    _pa_config: PaConfig
    use_overlay: bool
    pset_config: pathlib.Path | None
    grading_script: pathlib.Path | None

    CONFIG_NAME = ".parunconfig.json"

    def __init__(self,
                 grading_dir,
                 target_dir=".",
                 expected_dir=None,
                 overlay_dir=None,
                 use_overlay=True,
                 pset_config=None,
                 grading_script=None):

        self.grading_dir = pathlib.Path(grading_dir).resolve()
        self.target_dir = pathlib.Path(target_dir)

        def _rp(val, default):
            return pathlib.Path(val).resolve() if val is not None else default

        self.expected_dir = _rp(expected_dir, self.grading_dir / "expected")
        self.use_overlay = use_overlay
        self.overlay_dir = _rp(overlay_dir, self.grading_dir / "overlay")

        self.pset_config = pset_config

        if self.pset_config is None:
            self.pset_config = self.grading_dir / "config.json"

        self._pa_config = PaConfig.from_json_file(self.pset_config)

        self.grading_script = grading_script
        if self.grading_script is None:
            _grading_script = self.grading_dir / "grade.php"
            if _grading_script.exists():
                self.grading_script = _grading_script


    def all_commands(self) -> list[str]:
        return list(self._pa_config.runners.keys())

    def has_command(self, command: str) -> bool:
        return command in self.all_commands()

    def get_runner(self, command) -> PaRunner:
        if not self.has_command(command):
            raise ValueError(f"No runner named f{command}")

        return self._pa_config.runners[command]

    @classmethod
    def load_from_file(cls, config_file):
        with open(config_file, "r") as json_fd:
            jd = json.load(json_fd)
            if "grading_dir" not in jd:
                raise ValueError("PA run config founr at {} does not contain `grading_dir`  attribute".format(config_file))
            _grading_dir = pathlib.Path(jd["grading_dir"])
            if not _grading_dir.is_absolute():
                _grading_dir = (pathlib.Path(config_file).parent / _grading_dir).resolve()
                jd["grading_dir"] = _grading_dir

            return RunConfig(**jd)

    @classmethod
    def load_config(cls, start_dir="."):
        dir_to_check = pathlib.Path(start_dir).resolve()
        if dir_to_check.is_file():
            return cls.load_from_file(str(dir_to_check))

        config = None
        while True:
            config_path = dir_to_check / cls.CONFIG_NAME
            if config_path.exists():
                config = cls.load_from_file(config_path)
            dir_to_check = dir_to_check.parent
            if str(dir_to_check) == dir_to_check.root:
                break

        if config is None:
            import pdb; pdb.set_trace()
            raise ValueError("No config found")

        return config


def has_cmd(cmd_name):
    output, rv = do_exec("which {}".format(cmd_name), shell=True, check=False, return_rv=True)
    return rv == 0


def do_exec(cmd, check=True, shell=True, cwd=None, return_rv=False):
    global VERBOSE_MODE

    if VERBOSE_MODE:
        msg("Executing:  {}".format(" ".join(cmd) if isinstance(cmd, list) else cmd))

    proc = subprocess.run(" ".join(cmd) if shell and isinstance(cmd, list) else cmd, shell=shell, text=True,
                          stdout=subprocess.PIPE, stderr=subprocess.STDOUT, cwd=cwd)

    if check and proc.returncode != 0:
        do_exit(f"Command exited with {proc.returncode}:  {proc.stdout}")

    output = proc.stdout

    if return_rv:
        return output, proc.returncode
    else:
        return output

def do_exec_live(cmd, log_file=None, shell=True, check=False, attach=False, cwd=None):
    global VERBOSE_MODE

    if VERBOSE_MODE:
        msg("Executing:  {}".format(" ".join(cmd) if isinstance(cmd, list) else cmd))

    def _become_tty_fg():
        os.setpgrp()
        hdlr = signal.signal(signal.SIGTTOU, signal.SIG_IGN)
        tty = os.open("/dev/tty", os.O_RDWR)
        os.tcsetpgrp(tty, os.getpgrp())
        signal.signal(signal.SIGTTOU, hdlr)

    if (log_file is None) and (not attach):
        raise ValueError("Log file must be defined for non-interactive process")

    log_fd = open(str(log_file), "wb") if (not attach) else None

    cmd_to_run = " ".join(cmd) if shell and isinstance(cmd, list) else cmd
    kwargs = {}
    if attach:
        kwargs["preexec_fn"] = _become_tty_fg
    else:
        kwargs["stdout"] = subprocess.PIPE
        kwargs["stderr"] = subprocess.STDOUT
        kwargs["bufsize"] = 128

    proc = subprocess.Popen(cmd_to_run, shell=shell, cwd=cwd,
                            **kwargs)

    if (not attach):
        for line in proc.stdout:
            sys.stdout.buffer.write(line)
            sys.stdout.flush()
            log_fd.write(line)

    proc.wait()
    if log_fd is not None:
        log_fd.close()

    if check and proc.returncode != 0:
        do_exit(f"Command exited with {proc.returncode}:  {proc.stdout}")

    return proc


class RunInstance():
    suite_name: str
    config: RunConfig
    submission_dir: pathlib.Path
    work_path: pathlib.Path
    work_tempfile: tempfile.TemporaryDirectory | None = None
    preserve_work_dir: bool = False
    submission_is_repo: bool = False
    output_dir: pathlib.Path
    log_file: pathlib.Path
    notes_json: pathlib.Path
    results_json: pathlib.Path
    runner_json: pathlib.Path
    expected_results: pathlib.Path | None
    runner_rv: int

    RV_UNSET = -999

    def __init__(self,
                 suite_name: str,
                 config: RunConfig,
                 submission_dir: pathlib.Path,
                 work_dir: pathlib.Path | None=None,
                 output_dir: pathlib.Path | None=None,
                 expected_results: pathlib.Path | None = None,
                 preserve_work_dir=False,
                 submission_is_repo=False):
        self.suite_name = suite_name
        self.config = config
        self.submission_dir = submission_dir

        self.preserve_work_dir = preserve_work_dir
        self.submission_is_repo = submission_is_repo
        if work_dir is None:
            #self.work_tempfile = tempfile.TemporaryDirectory(delete=(not preserve_work_dir))
            #self.work_path = pathlib.Path(self.work_tempfile.name)
            self.work_path = DEFAULT_WORK_DIR
        else:
            self.work_path = pathlib.Path(work_dir)

        if output_dir is not None:
            self.output_dir = output_dir
        else:
            self.output_dir = self.work_path #/ "output"

        self.expected_results = pathlib.Path(expected_results) if expected_results is not None else None

        self.log_file = self.output_dir / "output.log"
        self.notes_json = self.output_dir / "pa_notes.json"
        self.results_json = self.output_dir / "results.json"
        self.runner_json = JSON_RESULTS_DEFAULT

        if (self.work_path == DEFAULT_WORK_DIR) and self.work_path.exists():
            shutil.rmtree(str(self.work_path))

        mkdir(str(self.work_path))
        self.clear_output_files()
        self.runner_rv = self.RV_UNSET

    def dir_name(self):
        return self.submission_dir.resolve().stem

    def cleanup(self):
        # if self.work_tempfile and (not self.preserve_work_dir):
        #     self.work_tempfile.cleanup()
        pass

    def clear_output_files(self):
        self.notes_json.unlink(missing_ok=True)
        self.runner_json.unlink(missing_ok=True)
        self.log_file.unlink(missing_ok=True)
        self.results_json.unlink(missing_ok=True)

    def _exec(self, cmd, check=True, shell=True):
        return do_exec(cmd, check=check, shell=shell, cwd=str(self.work_path))

    def get_expected_file(self, command=None):
        if self.expected_results is not None:
            assert (self.expected_results.exists())
            return self.expected_results

        if command is None:
            raise ValueError("Must specify command to search for expected output")

        target = self.config.expected_dir / self.dir_name() / "{}.json".format(command)
        return target

    def get_run_command(self, command):
        runner = self.config.get_runner(command)
        return runner.command

    def do_run(self, command):
        root_work_path = pathlib.Path("repo")
        #pset_path = root_work_path / self.config._pa_config.directory
        if self.submission_is_repo:
            self._exec(f"cp -pTRv {self.submission_dir} {str(root_work_path)}")
        else:
            pset_path = root_work_path / str(self.config.target_dir)
            self._exec(f"mkdir -pv {str(pset_path)}")
            self._exec(f"cp -pTRv {self.submission_dir} {str(pset_path)}")
        self._exec(f"mkdir -pv {str(self.output_dir)}")

        if self.config.use_overlay:
            self._exec(f"cp -pTRv {str(self.config.overlay_dir)} {str(root_work_path)}")
            self._exec("touch config.sh")
            self._exec("touch config.mk")

        runner = self.config.get_runner(command)
        if runner.command is None:
            import pdb; pdb.set_trace()
            raise ValueError("YO")

        #interactive = runner.is_interactive()
        interactive = False
        proc = do_exec_live(runner.command, str(self.log_file), shell=True,
                            cwd=str(self.work_path), attach=interactive)

        rv = proc.returncode
        if rv != 0 or VERBOSE_MODE:
            msg("Runner exited with status {}".format(rv))
            msg("Checkoff run '{}':  {}".format(command, _status(rv == 0)))

        return rv

    def runner_ok(self):
        assert (self.runner_rv != self.RV_UNSET)
        return (self.runner_rv == 0) or (self.runner_json.exists())

    def do_notes(self, command):
        grading_script = self.config.grading_script
        if grading_script is None:
            msg("No grading script for this pset, skipping notes pass".format(command),
                color=c.WARNING)
            return

        runner = self.config.get_runner(command)
        if runner.eval is None:
            msg("No grading function found for {}, skipping notes pass".format(command),
                color=c.OKBLUE)
            return

        cmd = [
            "php",
            str(PA_GRADING_RUNNER),
            f"-p{str(self.config.pset_config)}",
            f"-f{str(self.log_file)}",
            f"-o{str(self.notes_json)}",
            str(self.config.grading_script),
            "'{}'".format(runner.eval),
        ]
        with tempfile.NamedTemporaryFile() as log_fd:
            do_exec_live(cmd, log_fd.name, shell=True,
                         cwd=str(self.work_path), attach=False, check=True)
        if not self.notes_json.exists():
            msg("WARNING:  No notes file found at {}".format(str(self.notes_json)),
                color=c.WARNING)

    def do_process_results(self, command):

        runner_results = self.runner_json
        if runner_results.exists():
            results = PAResults.from_runner_json_file(runner_results, suite=self.suite_name)
        elif not self.runner_ok():
            results = PAResults.from_log(self.log_file, self.runner_rv, suite=self.suite_name)
            if VERBOSE_MODE:
                msg("Warning:  no per-test results found, creating from log file due to error")
        else:
            msg("Warning:  no per-test results found, can only compare based on exit status")
            assert (self.runner_rv != self.RV_UNSET)
            results = PAResults.from_empty(suite=self.suite_name)
            results.runner_rv = self.runner_rv

        results.add_grading_rubric(self.config._pa_config.grades)

        if self.notes_json.exists():
            results.add_notes_file(self.notes_json)

        results.write_json(self.results_json)
        if VERBOSE_MODE:
            msg("Wrote results to {}".format(str(self.results_json)))

        return results

    def run_checkoff(self, command):
        print_status("checkoff", "Running command {}".format(command))
        rv = self.do_run(command)
        self.runner_rv = rv
        print_status("checkoff", "Checkoff phase completed, runner exited with status {}".format(self.runner_rv))

        print_status("notes", "Starting notes pass")
        self.do_notes(command)
        print_status("notes", "Notes pass completed")

        results = self.do_process_results(command)

        # Default status:  did the runner succeed?
        run_ok = self.runner_ok()

        return results, run_ok

    @classmethod
    def do_check_expected(cls,
                          test_results_file: pathlib.Path,
                          expected_file: pathlib.Path,
                          show=True, error_on_fail=True):
        diff_output = None
        rv = None

        if not test_results_file.exists():
            msg("FAILURE:  Runner did not produce test results file at {}.  Runner may have aborted before it could write results.".format(str(test_results_file)), c.FAIL)
            return False

        diff_cmd = [
            "diff", "-urN", str(test_results_file), str(expected_file),
        ]

        diff_output, rv = do_exec(diff_cmd, check=False, shell=True, return_rv=True)
        ok = (rv == 0)
        if show:
            if ok:
                msg("{}:  Results match expected results".format(_status(rv == 0)))
            else:
                msg("{}{}{}:  Test results did not match expected results"
                    .format(c.FAIL if error_on_fail else c.WARNING,
                            "ERROR" if error_on_fail else "WARNING",
                            c.ENDC))
                msg(diff_output)
                msg("Expected results: {}".format(str(expected_file)))
                msg("Results this run: {}".format(str(test_results_file)))

        return ok

# def compare_results(actual_json: pathlib.Path, expected_json: pathlib.Path):
#     assert(actual_json.exists())
#     assert(expected_json.exists())

#     r_actual = PAResults.from_json_file(actual_json)
#     r_expected = PAResults.from_json_file(expected_json)


CHECK_EXPECTED_NONE = "skip"
CHECK_EXPECTED_WARN = "warn"
CHECK_EXPECTED_ERROR = "error"
CHECK_EXPECTED_CHOICES = [
    CHECK_EXPECTED_NONE,
    CHECK_EXPECTED_WARN,
    CHECK_EXPECTED_ERROR,
]

def main(input_args):
    global VERBOSE_MODE

    parser = argparse.ArgumentParser()
    parser.add_argument("--verbose", action="store_true")
    parser.add_argument("--run-config", type=str, default=None)
    parser.add_argument("--suite", type=str, default=None)
    parser.add_argument("--no-overlay", action="store_true")
    parser.add_argument("--work-dir", type=str, default=None)
    parser.add_argument("--output-dir", type=str, default=None)
    parser.add_argument("--no-cleanup", action="store_true", default=False)
    parser.add_argument("--submission-is-repo", action="store_true", default=False)
    #parser.add_argument("--skip-expected", action="store_true", default=False)
    parser.add_argument("--run-only", action="store_true", default=False)
    parser.add_argument("--run-expect-rv", type=int, default=0)
    parser.add_argument("--check-expected", action="store_true", default=False)
    parser.add_argument("--expect-all-fail", action="store_true", default=False)
    parser.add_argument("--expect-any", action="store_true", default=False)
    parser.add_argument("--expected-file", type=str, default=None,
                        help="Expected test.json file")
    parser.add_argument("--save-expected", action="store_true", default=False)
    parser.add_argument("--overall-result", type=str, default=None)
    parser.add_argument("submission_dir", type=str, default=None)
    parser.add_argument("command", type=str, default="list")

    args = parser.parse_args(input_args)

    submission_path = pathlib.Path(args.submission_dir).resolve()
    start_dir = args.run_config if args.run_config is not None else submission_path
    config = RunConfig.load_config(start_dir)
    suite_name = args.suite if args.suite is not None else submission_path.stem

    if args.verbose:
        VERBOSE_MODE = True

    if args.no_overlay:
        config.use_overlay = False

    command = args.command
    save_expected = args.save_expected
    available_commands = config.all_commands()

    if command == "list" or command not in available_commands:
        print("Available commands:\n{}".format("\n".join(["\t{}".format(c) for c in available_commands])))
        sys.exit(1)

    if args.submission_dir is None:
        print("Must specify submission dir")
        sys.exit(1)

    def _setup_dir(d):
        if d is not None:
            _d = pathlib.Path(d).resolve()
            mkdir(_d)
            return _d
        else:
            return None

    work_dir = _setup_dir(args.work_dir)
    output_dir = _setup_dir(args.output_dir)
    expected_results = pathlib.Path(args.expected_file) if args.expected_file else None
    use_expected = args.check_expected or (expected_results is not None)

    run = RunInstance(suite_name, config, submission_path,
                      work_dir=work_dir,
                      output_dir=output_dir,
                      expected_results=expected_results,
                      preserve_work_dir=args.no_cleanup,
                      submission_is_repo=args.submission_is_repo)

    compare_results = run.output_dir / COMPARE_RESULTS_NAME
    if compare_results.exists():
        compare_results.unlink()

    ok = True
    try:
        results, run_ok = run.run_checkoff(command)
        results.show()

        expected_file = expected_results if expected_results is not None else run.get_expected_file(command)

        if save_expected:
            msg("Writing results to expected file {}".format(_prels(expected_file)),
                color=c.OKCYAN)
            mkdir(expected_file.parent)
            print(do_exec(["cp", "-v", str(run.runner_json), str(expected_file)]))

        def _print_set_expected():
            print(c.WARNING + "If you believe the current test results are accurate" + c.ENDC + ", you can set the expected \n"
                  "results by copying the test results to the repo at the appropriate path, e.g.:\n"
                   "\t{}\n"
 "\tcp -v {} {}\n"
                   .format(run.get_run_command(command),
                           JSON_RESULTS_DEFAULT, _prels(expected_file)))

        skip_reason = None
        cr: CompareResult|None = None
        expected_ok = False

        # if args.skip_expected:
        #     print_status("check_expected", "Expected results check skipped based on command-line arguments")
        #     skip_reason = "Disabled by command line"
        #     use_expected = False


        if args.check_expected:
            if expected_file.exists():
                print("Attempting to use expected results at {}".format(_prels(expected_file)))
                use_expected = True
            else:
                print(" expected results at {}".format(_prels(expected_file)))
                print_status("check_expected", "No expected results found, skipping check")
                skip_reason = "No baseline results found"
                use_expected = False

        if use_expected:
            cr = CompareResult.from_files(run.runner_json, expected_file, suite=suite_name)
            expected_ok = cr.print_summary()
            if expected_ok:
                print_status("check_expected",
                             "{}:  results match expected results".format(_status(expected_ok)))
            else:
                print_status("check_expected",
                             "{}:  results differ from expected results".format(_status(expected_ok)))

        msg("\n++++++++++++++++++++++ PA_RUN SUMMARY ++++++++++++++++++++++", color=c.OKCYAN)
        results.show()
        if use_expected and cr is not None:
            cr.print_summary(summary_only=True)
            cr.write_json(compare_results)

        ok = False

        if use_expected:
            ok = cr.is_passing()
        elif args.expect_all_fail:
            ok = results.has_tests() and results.is_all_failing()
        elif args.expect_any:
            ok = results.has_tests()
        elif args.run_only:
            ok = (results.runner_rv == args.run_expect_rv)
        else:
            ok = results.has_tests() and results.is_all_passing()

        if not results.has_tests():
            print("\nOverall result: {}{}{} (no JSON-based test data present)".format(c.WARNING, "UNKNOWN", c.ENDC))
            print("Runner exit code:  {}".format(results.runner_rv))
        else:
            if args.expect_any:
                print("\nOverall result:  {} {}"
                      .format(c.color("UNKNOWN", c.WARNING) if ok else c.color("FAIL", c.FAIL),
                              "(pass/fail check disabled)"))
            else:
                print("\nOverall result:  {} {}"
                      .format(c.color("PASS", c.OKGREEN) if ok else c.color("FAIL", c.FAIL),
                              "(all test failures expected)" if args.expect_all_fail else ""))
            if use_expected and (not cr.is_passing()):
                _print_set_expected()

        def _exists(x: pathlib.Path | None):
            if x is None:
                return False
            else:
                return x.exists()

        print("")
        files_to_show = [
            ("Log file", run.log_file, _exists),
            ("PA notes", run.notes_json, _exists),
            ("Full test results", run.results_json, _exists),
            ("Compare results", compare_results, _exists),
        ]
        for x in files_to_show:
            name, path, proc = x
            if proc(path):
               print("{:20s} {}".format(name, _prels(path)))

        if args.overall_result:
            result_path = pathlib.Path(args.overall_result).resolve()
            mkdir(result_path.parent)
            overall_result_src = compare_results if compare_results.exists() else run.results_json
            msg("Writing results to {}".format(_prels(result_path)),
                color=c.OKCYAN)
            print(do_exec(["cp", "-v", str(overall_result_src), str(result_path)]))


    finally:
        run.cleanup()

    msg("++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++", color=c.OKCYAN)
    return 0 if ok else 1

if __name__ == "__main__":
    sys.exit(main(sys.argv[1:]))
